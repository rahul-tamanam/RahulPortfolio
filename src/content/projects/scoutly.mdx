---
name: Prompt Fuzzing Framework
description: A modular system to stress-test LLM safety and alignment
github: https://github.com/VedanshAvlani2/Prompt-Fuzzing-Framework
techstack:
  - Python 3.11
  - LM Studio
  - Hugging Face Transformers
  - PyTorch
  - HTML / Plotly / Bootstrap 
selected: true
dateCreated: '2026-12-02T00:00:00Z'
---

The Prompt Fuzzing Framework is designed to automatically mutate, test, and analyze prompts against large language models (LLMs) to uncover unsafe, misaligned, or policy-violating behaviors.
It provides a secure sandbox, automated detectors, and a triage interface to benchmark and visualize vulnerabilities across models.